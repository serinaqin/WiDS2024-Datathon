{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65862,"databundleVersionId":7469115,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_columns', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-13T20:50:35.580216Z","iopub.execute_input":"2024-02-13T20:50:35.580612Z","iopub.status.idle":"2024-02-13T20:50:35.586091Z","shell.execute_reply.started":"2024-02-13T20:50:35.580576Z","shell.execute_reply":"2024-02-13T20:50:35.585128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/widsdatathon2024-challenge1/training.csv\")\ntest = pd.read_csv(\"/kaggle/input/widsdatathon2024-challenge1/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:35.587817Z","iopub.execute_input":"2024-02-13T20:50:35.588106Z","iopub.status.idle":"2024-02-13T20:50:35.898585Z","shell.execute_reply.started":"2024-02-13T20:50:35.588079Z","shell.execute_reply":"2024-02-13T20:50:35.897470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:35.899873Z","iopub.execute_input":"2024-02-13T20:50:35.900195Z","iopub.status.idle":"2024-02-13T20:50:35.904720Z","shell.execute_reply.started":"2024-02-13T20:50:35.900167Z","shell.execute_reply":"2024-02-13T20:50:35.903740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Cleaning**","metadata":{}},{"cell_type":"code","source":"# group region info\nregion_info = train.columns[15:82].tolist()\nprint(region_info)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:35.906678Z","iopub.execute_input":"2024-02-13T20:50:35.907005Z","iopub.status.idle":"2024-02-13T20:50:35.915149Z","shell.execute_reply.started":"2024-02-13T20:50:35.906977Z","shell.execute_reply":"2024-02-13T20:50:35.914068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check missing data**","metadata":{}},{"cell_type":"code","source":"# check count & % of missing values in the dataset\nmissing_info = train.isna().mean() * 100\nmissing_info = missing_info[missing_info > 0].sort_values(ascending=False)\n#Font setting\nORANGE, BOLD, RESET = '\\033[93m', '\\033[1m', '\\033[0m'\n\nfor column, missing_percentage in missing_info.items():\n    print(f\"{BOLD}{column}{RESET} has {BOLD}{ORANGE}{train[column].isna().sum()}{RESET} missing values, which is {BOLD}{ORANGE}{missing_percentage:.2f}%{RESET} of the column.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:35.916513Z","iopub.execute_input":"2024-02-13T20:50:35.917593Z","iopub.status.idle":"2024-02-13T20:50:35.956938Z","shell.execute_reply.started":"2024-02-13T20:50:35.917554Z","shell.execute_reply":"2024-02-13T20:50:35.955955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing values of these columns are in the same rows\n\ncolumns_to_check = [\n    'income_household_75_to_100', 'income_household_150_over', 'income_household_15_to_20',\n    'income_household_20_to_25', 'income_household_25_to_35', 'income_household_35_to_50',\n    'income_household_50_to_75', 'income_household_100_to_150', 'income_household_six_figure',\n    'income_household_under_5', 'home_ownership', 'home_value', 'rent_median', 'rent_burden',\n    'farmer', 'self_employed', 'income_household_5_to_10', 'income_household_10_to_15',\n    'income_household_median', 'family_dual_income', 'limited_english', 'poverty', 'family_size'\n]\n\n# Initialize the missing values filter as False for all rows\nmissing_values_filter = pd.Series([False] * len(train))\n\nfor col in columns_to_check:\n    missing_values_filter |= train[col].isnull()\n\nmissing_values = train[missing_values_filter]\n\n# missing_values.to_csv('/kaggle/working/missing_values.csv', index=False)\n# pd.read_csv(\"/kaggle/working/missing_values.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:35.958148Z","iopub.execute_input":"2024-02-13T20:50:35.958442Z","iopub.status.idle":"2024-02-13T20:50:35.972573Z","shell.execute_reply.started":"2024-02-13T20:50:35.958415Z","shell.execute_reply":"2024-02-13T20:50:35.971699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing values of these columns are in the same row\ncolumns_to_check = [\n    'PM25', 'Ozone', 'N02'\n]\n\n# Initialize the missing values filter as False for all rows\nmissing_values_filter = pd.Series([False] * len(train))\n\nfor col in columns_to_check:\n    missing_values_filter |= train[col].isnull()\n\nmissing_values = train[missing_values_filter]\n\n# missing_values.to_csv('/kaggle/working/missing_values3.csv', index=False)\n# pd.read_csv(\"/kaggle/working/missing_values3.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:35.974023Z","iopub.execute_input":"2024-02-13T20:50:35.974308Z","iopub.status.idle":"2024-02-13T20:50:35.981424Z","shell.execute_reply.started":"2024-02-13T20:50:35.974282Z","shell.execute_reply":"2024-02-13T20:50:35.980663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing values of these columns are in the same row\ncolumns_to_check = [\n    'race_native', 'race_white', 'labor_force_participation', 'unemployment_rate', 'population',\n    'density', 'veteran', 'health_uninsured', 'commute_time', 'education_college_or_above',\n    'race_pacific', 'race_black', 'disabled', 'hispanic', 'race_asian', 'race_multiple',\n    'race_other', 'education_stem_degree', 'age_under_10', 'education_graduate', 'education_bachelors',\n    'age_20s', 'age_30s', 'age_40s', 'age_50s', 'age_60s', 'age_70s', 'age_over_80', 'male', 'female',\n    'married', 'divorced', 'never_married', 'widowed', 'age_median', 'income_individual_median',\n    'age_10_to_19', 'education_less_highschool', 'education_highschool', 'education_some_college',\n    'housing_units'\n]\n\n# Initialize the missing values filter as False for all rows\nmissing_values_filter = pd.Series([False] * len(train))\n\nfor col in columns_to_check:\n    missing_values_filter |= train[col].isnull()\n\nmissing_values = train[missing_values_filter]\n\n# missing_values.to_csv('/kaggle/working/missing_values2.csv', index=False)\n# pd.read_csv(\"/kaggle/working/missing_values2.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:35.982510Z","iopub.execute_input":"2024-02-13T20:50:35.983022Z","iopub.status.idle":"2024-02-13T20:50:36.002382Z","shell.execute_reply.started":"2024-02-13T20:50:35.982980Z","shell.execute_reply":"2024-02-13T20:50:36.001315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop columns and rows**","metadata":{}},{"cell_type":"code","source":"# # drop rows with many missing values\n# drop_id = [224030, 514282, 387901, 224030, 411586]\ndrop_id = [224030]\ntrain = train[~train['patient_id'].isin(drop_id)]\n\n# check if they're dropped\nremaining = train['patient_id'].isin(drop_id).any()\nprint(remaining)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:36.006358Z","iopub.execute_input":"2024-02-13T20:50:36.006653Z","iopub.status.idle":"2024-02-13T20:50:36.019807Z","shell.execute_reply.started":"2024-02-13T20:50:36.006626Z","shell.execute_reply":"2024-02-13T20:50:36.018099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop columns in list due to high percentage of na values\n# drop description owing to duplicatin of diagnosis code \ndrop_cols=['patient_race', 'payer_type', 'bmi','metastatic_first_novel_treatment','metastatic_first_novel_treatment_type', 'breast_cancer_diagnosis_desc']\ntrain.drop(drop_cols, axis=1, inplace=True)\ntest.drop(drop_cols, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:36.020884Z","iopub.execute_input":"2024-02-13T20:50:36.021181Z","iopub.status.idle":"2024-02-13T20:50:36.031540Z","shell.execute_reply.started":"2024-02-13T20:50:36.021150Z","shell.execute_reply":"2024-02-13T20:50:36.030527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clean dirty data of state, region, division based on zip code**","metadata":{}},{"cell_type":"code","source":"# combine region, division, state info to state column\ntrain['state'] = train['Region'] + ', ' + train['Division'] + ', ' + train['patient_state']\ndrop_cols = ['Region', 'Division', 'patient_state']\ntrain.drop(drop_cols, axis=1, inplace=True)\n#print(train['state'])\n\ntest['state'] = test['Region'] + ', ' + test['Division'] + ', ' + test['patient_state']\ndrop_cols = ['Region', 'Division', 'patient_state']\ntest.drop(drop_cols, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:36.032902Z","iopub.execute_input":"2024-02-13T20:50:36.033897Z","iopub.status.idle":"2024-02-13T20:50:36.059792Z","shell.execute_reply.started":"2024-02-13T20:50:36.033865Z","shell.execute_reply":"2024-02-13T20:50:36.058971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assume zip code is true, clean state value\n\ndef safe_mode(x):\n    modes = pd.Series.mode(x)\n    if not modes.empty:\n        return modes[0]\n    else:\n        return None\n\n# Find the most common 'state' for each 'zip code'\nmost_common_state_train = train.groupby('patient_zip3')['state'].agg(safe_mode)\nmost_common_state_test = test.groupby('patient_zip3')['state'].agg(safe_mode)\n\n# Map each 'zip code' to its most common 'state'\ntrain['most_common_state'] = train['patient_zip3'].map(most_common_state_train)\ntest['most_common_state'] = test['patient_zip3'].map(most_common_state_test)\n\n# Replace 'state' values if there is the most common 'state', else keep original\ntrain['state'] = train.apply(lambda x: x['most_common_state'] \n                             if pd.notnull(x['most_common_state']) \n                             else x['state'], axis=1)\ntest['state'] = test.apply(lambda x: x['most_common_state'] \n                           if pd.notnull(x['most_common_state']) \n                           else x['state'], axis=1)\n    \n# Drop the temporary 'most_common_state' column\ntrain.drop('most_common_state', axis=1, inplace=True)\ntest.drop('most_common_state', axis=1, inplace=True)\n\n# print(train)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:36.060971Z","iopub.execute_input":"2024-02-13T20:50:36.061790Z","iopub.status.idle":"2024-02-13T20:50:36.545700Z","shell.execute_reply.started":"2024-02-13T20:50:36.061751Z","shell.execute_reply":"2024-02-13T20:50:36.544737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analyze the numerical data to be filled (excluding the 1 row)**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Plots suggest that not every column can be filled with mean value\n\ncolumns_to_analyze = [\n    'PM25', 'Ozone', 'N02', \n    'income_household_75_to_100', 'income_household_150_over', \n    'income_household_15_to_20', 'income_household_20_to_25', \n    'income_household_25_to_35', 'income_household_35_to_50', \n    'income_household_50_to_75', 'income_household_100_to_150', \n    'income_household_six_figure', 'income_household_under_5', \n    'home_ownership', 'home_value', 'rent_median', 'rent_burden', \n    'farmer', 'self_employed', 'income_household_5_to_10', \n    'income_household_10_to_15', 'income_household_median', \n    'family_dual_income', 'limited_english', 'poverty', 'family_size'\n]\n# for column in columns_to_analyze:\n#     if column in train.columns:\n#         print(f\"Analyzing {column}:\")\n\n#         # Histogram\n#         plt.hist(train[column].dropna(), bins=30)\n#         plt.title(f\"Histogram of {column}\")\n#         plt.show()\n\n#         # Box Plot\n#         sns.boxplot(x=train[column])\n#         plt.title(f\"Box Plot of {column}\")\n#         plt.show()\n\n#         # Shapiro-Wilk Test\n#         stat, p = stats.shapiro(train[column].dropna())\n#         print('Shapiro-Wilk Test: Statistics=%.3f, p=%.3f' % (stat, p))\n\n#         # Skewness and Kurtosis\n#         print(f\"Skewness of {column}: {train[column].skew()}\")\n#         print(f\"Kurtosis of {column}: {train[column].kurt()}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:36.547012Z","iopub.execute_input":"2024-02-13T20:50:36.547387Z","iopub.status.idle":"2024-02-13T20:50:36.557426Z","shell.execute_reply.started":"2024-02-13T20:50:36.547352Z","shell.execute_reply":"2024-02-13T20:50:36.556531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fill rest missing numerical data with mean / median**","metadata":{}},{"cell_type":"code","source":"# if normally distributed, fill with mean, if not, fill with median\nfrom scipy import stats\nimport numpy as np\n\ndef fill_missing_values(df, column):\n    # Remove NA values for the test\n    clean_data = df[column].dropna()\n    \n    # Normalize data for the Kolmogorov-Smirnov test\n    normalized_data = (clean_data - clean_data.mean()) / clean_data.std()\n\n    # Perform Kolmogorov-Smirnov test for normality\n    d, p = stats.kstest(normalized_data, 'norm')\n    \n    # If data is normally distributed (p > 0.05), use mean; else use median\n    if p > 0.05:\n        fill_value = clean_data.mean()\n    else:\n        fill_value = clean_data.median()\n\n    # Fill missing values\n    df[column].fillna(fill_value, inplace=True)\n\nfor column in columns_to_analyze:\n    fill_missing_values(train, column)\n    fill_missing_values(test, column)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:36.558577Z","iopub.execute_input":"2024-02-13T20:50:36.558964Z","iopub.status.idle":"2024-02-13T20:50:37.254241Z","shell.execute_reply.started":"2024-02-13T20:50:36.558934Z","shell.execute_reply":"2024-02-13T20:50:37.253319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check count & % of missing values in the dataset\nmissing_info = train.isna().mean() * 100\nmissing_info = missing_info[missing_info > 0].sort_values(ascending=False)\n#Font setting\nORANGE, BOLD, RESET = '\\033[93m', '\\033[1m', '\\033[0m'\n\nfor column, missing_percentage in missing_info.items():\n    print(f\"{BOLD}{column}{RESET} has {BOLD}{ORANGE}{train[column].isna().sum()}{RESET} missing values, which is {BOLD}{ORANGE}{missing_percentage:.2f}%{RESET} of the column.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:37.255350Z","iopub.execute_input":"2024-02-13T20:50:37.255648Z","iopub.status.idle":"2024-02-13T20:50:37.267215Z","shell.execute_reply.started":"2024-02-13T20:50:37.255620Z","shell.execute_reply":"2024-02-13T20:50:37.266090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # missing \"Region\" \"Division\" and \"patient_state\" are in the same rows\n\n# # missing_values_filter = train['Region'].isnull() | train['Division'].isnull() | train['patient_state'].isnull()\n# missing_values_filter = train['state'].isnull()\n# rows_with_missing_values = train[missing_values_filter]\n\n# # print(rows_with_missing_values)\n# # print(rows_with_missing_values.describe())\n\n# rows_with_missing_values.to_csv('/kaggle/working/rows_with_missing_values.csv', index=False)\n# pd.read_csv(\"/kaggle/working/rows_with_missing_values.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:37.268863Z","iopub.execute_input":"2024-02-13T20:50:37.269714Z","iopub.status.idle":"2024-02-13T20:50:37.274310Z","shell.execute_reply.started":"2024-02-13T20:50:37.269653Z","shell.execute_reply":"2024-02-13T20:50:37.273350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Align new and old version of diagnosis code**","metadata":{}},{"cell_type":"code","source":"# print(train.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:37.275545Z","iopub.execute_input":"2024-02-13T20:50:37.276095Z","iopub.status.idle":"2024-02-13T20:50:37.282673Z","shell.execute_reply.started":"2024-02-13T20:50:37.276057Z","shell.execute_reply":"2024-02-13T20:50:37.281858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['breast_cancer_diagnosis_code'] = train['breast_cancer_diagnosis_code'].astype(str)\ntest['breast_cancer_diagnosis_code'] = test['breast_cancer_diagnosis_code'].astype(str)\ndictionary = {'1741':'C5011', '1742':'C5021', '1743':'C5031', '1744':'C5041', '1745':'C5051',\n              '1746':'C5061', '1748':'C5081', '1749':'C5091', '1759':'C50929', '19881':'C7981'}\n\nfor key, value in dictionary.items():\n    train['breast_cancer_diagnosis_code'] = train['breast_cancer_diagnosis_code'].replace(key, value)\n    test['breast_cancer_diagnosis_code'] = test['breast_cancer_diagnosis_code'].replace(key, value)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:37.283831Z","iopub.execute_input":"2024-02-13T20:50:37.284190Z","iopub.status.idle":"2024-02-13T20:50:37.320840Z","shell.execute_reply.started":"2024-02-13T20:50:37.284162Z","shell.execute_reply":"2024-02-13T20:50:37.319615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Reformatting","metadata":{}},{"cell_type":"code","source":"# diagnosis code can be splited to indicate different things 19881？\n# split breast_cancer_diagnosis_code to Type, Position, Gender, Laterity\n\ndef split_diagnosis_code(code):\n    if code.startswith('C50'):\n        return {\n            'breast_cancer_Type': 'C50',\n            'breast_cancer_Position': code[3] if len(code) > 3 else 'NA',\n            'Gender': code[4] if len(code) > 4 else 'NA',\n            'breast_cancer_Laterality': code[5] if len(code) > 5 else 'NA'\n        }\n    else:\n        return {\n            'breast_cancer_Type': code,\n            'breast_cancer_Position': 'NA',\n            'Gender': 'NA',\n            'breast_cancer_Laterality': 'NA'\n        }\n\n# Apply the function to each code\ntrain_split = train['breast_cancer_diagnosis_code'].apply(lambda x: pd.Series(split_diagnosis_code(x)))\ntest_split = test['breast_cancer_diagnosis_code'].apply(lambda x: pd.Series(split_diagnosis_code(x)))\n\n# Concatenate the original DataFrame with the new columns\ntrain = pd.concat([train, train_split], axis=1)\ntest = pd.concat([test, test_split], axis=1)\n\n# drop breast_cancer_diagnosis_code\ntrain = train.drop('breast_cancer_diagnosis_code', axis=1)\ntest = test.drop('breast_cancer_diagnosis_code', axis=1)\n\ngender_counts = train['Gender'].value_counts(normalize=True) * 100\n\nprint(gender_counts)\n\n# Since the patient_gender only has Female, we keep the Gender column cause it's more plausible\ntrain = train.drop('patient_gender', axis=1)\ntest = test.drop('patient_gender', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:37.322078Z","iopub.execute_input":"2024-02-13T20:50:37.322902Z","iopub.status.idle":"2024-02-13T20:50:42.368585Z","shell.execute_reply.started":"2024-02-13T20:50:37.322860Z","shell.execute_reply":"2024-02-13T20:50:42.367607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# diagnosis code can be splited to indicate different things 19881？\n# split metastatic_cancer_diagnosis_code to Type, Position, Gender, Laterity\n\ndef split_metastatic_code(code):\n    return {\n        'metastatic_cancer_Type': code[0:3],\n        'metastatic_cancer_Organ': code[3] if len(code) > 3 else 'NA',\n        'metastatic_cancer_Laterality': code[4] if len(code) > 4 else 'NA'\n    }\n\n# Apply the function to each code\ntrain_split = train['metastatic_cancer_diagnosis_code'].apply(lambda x: pd.Series(split_metastatic_code(x)))\ntest_split = test['metastatic_cancer_diagnosis_code'].apply(lambda x: pd.Series(split_metastatic_code(x)))\n\n# Concatenate the original DataFrame with the new columns\ntrain = pd.concat([train, train_split], axis=1)\ntest = pd.concat([test, test_split], axis=1)\n\n# drop breast_cancer_diagnosis_code\ntrain = train.drop('metastatic_cancer_diagnosis_code', axis=1)\ntest = test.drop('metastatic_cancer_diagnosis_code', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:42.369849Z","iopub.execute_input":"2024-02-13T20:50:42.370152Z","iopub.status.idle":"2024-02-13T20:50:47.250342Z","shell.execute_reply.started":"2024-02-13T20:50:42.370124Z","shell.execute_reply":"2024-02-13T20:50:47.249550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the 'state' column back to region, division and state\ntrain[['Region', 'Division', 'patient_state']] = train['state'].str.split(', ', expand=True)\ntrain = train.drop('state', axis=1)\n\ntest[['Region', 'Division', 'patient_state']] = test['state'].str.split(', ', expand=True)\ntest = test.drop('state', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.251537Z","iopub.execute_input":"2024-02-13T20:50:47.252168Z","iopub.status.idle":"2024-02-13T20:50:47.303875Z","shell.execute_reply.started":"2024-02-13T20:50:47.252135Z","shell.execute_reply":"2024-02-13T20:50:47.302198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding Categorical Variables","metadata":{}},{"cell_type":"code","source":"categorical_cols = train.select_dtypes(include=['object', 'category']).columns\nnumerical_cols = train.select_dtypes(include=['float64', 'int64']).columns.drop('DiagPeriodL90D')\n\nprint(categorical_cols.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.305551Z","iopub.execute_input":"2024-02-13T20:50:47.306092Z","iopub.status.idle":"2024-02-13T20:50:47.323195Z","shell.execute_reply.started":"2024-02-13T20:50:47.306046Z","shell.execute_reply":"2024-02-13T20:50:47.321831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate unique variables in a column\nfor col in categorical_cols:\n    unique_nums = train[col].nunique()\n    print(f\"Number of unique values in {col}: {unique_nums}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.324486Z","iopub.execute_input":"2024-02-13T20:50:47.324839Z","iopub.status.idle":"2024-02-13T20:50:47.346674Z","shell.execute_reply.started":"2024-02-13T20:50:47.324800Z","shell.execute_reply":"2024-02-13T20:50:47.345163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_cols_test = test.select_dtypes(include=['object', 'category']).columns\nnumerical_cols_test = test.select_dtypes(include=['float64', 'int64'])\n\nprint(categorical_cols_test.tolist())\nfor col in categorical_cols:\n    unique_nums = test[col].nunique()\n    print(f\"Number of unique values in {col}: {unique_nums}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.352696Z","iopub.execute_input":"2024-02-13T20:50:47.353666Z","iopub.status.idle":"2024-02-13T20:50:47.379222Z","shell.execute_reply.started":"2024-02-13T20:50:47.353621Z","shell.execute_reply":"2024-02-13T20:50:47.378299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop('patient_id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.380363Z","iopub.execute_input":"2024-02-13T20:50:47.380754Z","iopub.status.idle":"2024-02-13T20:50:47.391485Z","shell.execute_reply.started":"2024-02-13T20:50:47.380721Z","shell.execute_reply":"2024-02-13T20:50:47.390336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import category_encoders as ce\n\n# One-Hot Encoding\ntrain = pd.get_dummies(train, columns=['breast_cancer_Type', 'Region', 'Gender', 'breast_cancer_Laterality', 'metastatic_cancer_Type', 'metastatic_cancer_Laterality'])\ntest = pd.get_dummies(test, columns=['breast_cancer_Type', 'Region', 'Gender', 'breast_cancer_Laterality', 'metastatic_cancer_Type', 'metastatic_cancer_Laterality'])\n\n# Binary Encoding\nencoder = ce.BinaryEncoder(cols=['breast_cancer_Position', 'Division', 'patient_state', 'metastatic_cancer_Organ'])\ntrain = encoder.fit_transform(train)\ntest = encoder.transform(test)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.392816Z","iopub.execute_input":"2024-02-13T20:50:47.393198Z","iopub.status.idle":"2024-02-13T20:50:47.569570Z","shell.execute_reply.started":"2024-02-13T20:50:47.393124Z","shell.execute_reply":"2024-02-13T20:50:47.568470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check","metadata":{}},{"cell_type":"code","source":"non_numeric_columns = train.select_dtypes(exclude=['int', 'float', 'int64', 'float64']).columns\n\nprint(\"Non-numeric columns:\", non_numeric_columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.570918Z","iopub.execute_input":"2024-02-13T20:50:47.571238Z","iopub.status.idle":"2024-02-13T20:50:47.577447Z","shell.execute_reply.started":"2024-02-13T20:50:47.571209Z","shell.execute_reply":"2024-02-13T20:50:47.576468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n\n# # train = train.drop('patient_id', axis=1)\n\n# X = train.drop('DiagPeriodL90D', axis=1)  # Features\n# y = train['DiagPeriodL90D']  # Target\n\n# # Splitting the data into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\n# # Initialize RandomForest\n# clf = RandomForestClassifier(random_state=42)\n\n# # Train the model\n# clf.fit(X_train, y_train)\n\n# # Predict on the test set\n# # y_pred = clf.predict(X_test)\n\n# y_pred_proba = clf.predict_proba(X_test)\n\n# # probability -- the second column\n# y_pred_proba_class1 = y_pred_proba[:, 1]\n\n\n# # Evaluate the model\n# # print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.578897Z","iopub.execute_input":"2024-02-13T20:50:47.579531Z","iopub.status.idle":"2024-02-13T20:50:47.587039Z","shell.execute_reply.started":"2024-02-13T20:50:47.579490Z","shell.execute_reply":"2024-02-13T20:50:47.586028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom xgboost import XGBClassifier\n\nX = train.drop(['DiagPeriodL90D'], axis=1)\ny = train['DiagPeriodL90D']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\n# dtrain = xgb.DMatrix(X_train, label=y_train)\n# dtest = xgb.DMatrix(X_test, label=y_test)\n\n# params = {\n#     'max_depth': 6,\n#     'eta': 0.3,\n#     'objective': 'binary:logistic',\n#     'eval_metric': 'auc',  \n#     'seed': 42\n# }\n\n# evallist = [(dtest, 'eval'), (dtrain, 'train')]\n# num_round = 1000  # Number of boosting rounds\n# bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=10, verbose_eval=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.588484Z","iopub.execute_input":"2024-02-13T20:50:47.588804Z","iopub.status.idle":"2024-02-13T20:50:47.607946Z","shell.execute_reply.started":"2024-02-13T20:50:47.588769Z","shell.execute_reply":"2024-02-13T20:50:47.606715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xg_clf = xgb.XGBClassifier(objective='binary:logistic', seed=42)\n\n# param_grid = {\n#     'n_estimators': [100, 200],\n#     'learning_rate': [0.05, 0.1],\n#     'max_depth': [3, 5, 7],\n#     'colsample_bytree': [0.7, 1],\n#     'subsample': [0.7, 1]\n# }\n\n# grid_clf = GridSearchCV(xg_clf, param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n# grid_clf.fit(X_train, y_train)\n\n# print(f\"Best parameters found: {grid_clf.best_params_}\")\n\noptimal_params = {\n    'objective': 'binary:logistic',\n    'colsample_bytree': 1,\n    'learning_rate': 0.05,\n    'max_depth': 3,\n    'n_estimators': 200,\n    'subsample': 1,\n    'seed': 42,\n    'eval_metric': 'auc'\n}\n\noptimal_xgb = XGBClassifier(**optimal_params)\noptimal_xgb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:47.612026Z","iopub.execute_input":"2024-02-13T20:50:47.612326Z","iopub.status.idle":"2024-02-13T20:50:48.655861Z","shell.execute_reply.started":"2024-02-13T20:50:47.612298Z","shell.execute_reply":"2024-02-13T20:50:48.654767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny_pred_proba = optimal_xgb.predict_proba(X_test)[:, 1]\ny_pred = (y_pred_proba > 0.5).astype(int)\n\nprint(f\"Test ROC AUC: {roc_auc_score(y_test, y_pred_proba)}\")\nprint(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:48.657370Z","iopub.execute_input":"2024-02-13T20:50:48.658022Z","iopub.status.idle":"2024-02-13T20:50:48.693217Z","shell.execute_reply.started":"2024-02-13T20:50:48.657980Z","shell.execute_reply":"2024-02-13T20:50:48.692126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"# # test_predictions = clf.predict(test.drop('patient_id', axis=1))\n# test_probabilities = clf.predict_proba(test.drop('patient_id', axis=1))\n# test_predictions_proba_class1 = test_probabilities[:, 1]\n\n\n# # Create a submission DataFrame\n# submission = pd.DataFrame({\n#     'patient_id': test['patient_id'],\n#     'DiagPeriodL90D': test_predictions_proba_class1\n# })\n\n# # Write the submission DataFrame to a CSV file\n# submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:48.694367Z","iopub.execute_input":"2024-02-13T20:50:48.695180Z","iopub.status.idle":"2024-02-13T20:50:48.700098Z","shell.execute_reply.started":"2024-02-13T20:50:48.695148Z","shell.execute_reply":"2024-02-13T20:50:48.699164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"test_probabilities = optimal_xgb.predict_proba(test.drop('patient_id', axis=1))[:, 1]\n\nsubmission = pd.DataFrame({\n    'patient_id': test['patient_id'],\n    'DiagPeriodL90D': test_probabilities\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T20:50:48.701231Z","iopub.execute_input":"2024-02-13T20:50:48.702006Z","iopub.status.idle":"2024-02-13T20:50:48.792844Z","shell.execute_reply.started":"2024-02-13T20:50:48.701967Z","shell.execute_reply":"2024-02-13T20:50:48.792072Z"},"trusted":true},"execution_count":null,"outputs":[]}]}